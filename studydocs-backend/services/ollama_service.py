import requests
import json

OLLAMA_URL = "http://localhost:11434/api/generate"

def query_llama(prompt, model="llama3.2:1b"):
    """
    Envoie une requ√™te √† Ollama avec streaming et limites
    
    Args:
        prompt (str): Le prompt complet
        model (str): Le mod√®le √† utiliser
        
    Returns:
        str: La r√©ponse de l'IA
    """
    print(f"ü§ñ Appel √† Ollama ({model})...")
    
    # LIMITER le prompt √† 4000 caract√®res pour √©viter les timeouts
    if len(prompt) > 4000:
        print(f"‚ö†Ô∏è Prompt trop long ({len(prompt)}), tronqu√© √† 4000.")
        prompt = prompt[:4000]
    
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True,  # ACTIVER le streaming
        "options": {
            "num_ctx": 2048  # LIMITER la fen√™tre de contexte
        }
    }
    
    try:
        # Timeout r√©duit car on attend juste le d√©but du stream
        response = requests.post(OLLAMA_URL, json=payload, stream=True, timeout=60)
        response.raise_for_status()
        
        result = ""
        # Collecter la r√©ponse en streaming
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode('utf-8'))
                if 'response' in chunk:
                    result += chunk['response']
                    
        print("‚úì R√©ponse re√ßue")
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur Ollama : {str(e)}")
        return f"Erreur de connexion √† Ollama : {str(e)}. V√©rifiez qu'Ollama est bien lanc√© localement."

def format_prompt(query, context_chunks):
    """
    Formate le prompt final avec le contexte extrait des documents
    """
    # Limiter aussi le contexte s'il est trop grand
    max_context_length = 3000
    context_text = ""
    
    for c in context_chunks:
        chunk_text = f"--- Source: {c['metadata']['source']} ---\n{c['text']}\n\n"
        if len(context_text) + len(chunk_text) < max_context_length:
            context_text += chunk_text
        else:
            break
            
    prompt = f"""Tu es StudyDocs AI, un assistant d'√©tude intelligent.
Utilise les extraits de documents fournis ci-dessous pour r√©pondre √† la question de l'√©tudiant.
Si la r√©ponse n'est pas dans le contexte, dis-le poliment mais essaie d'aider quand m√™me.

CONTEXTE :
{context_text}

QUESTION DE L'√âTUDIANT :
{query}

R√âPONSE (Sois clair, p√©dagogique et structure ta r√©ponse avec du Markdown si n√©cessaire) :
"""
    return prompt
